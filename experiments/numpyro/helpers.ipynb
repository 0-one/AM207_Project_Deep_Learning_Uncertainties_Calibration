{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regular numpy in addition to JAX's numpy\n",
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "from jax import lax, vmap\n",
    "import jax.numpy as np\n",
    "import jax.random as random\n",
    "\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro import handlers\n",
    "from numpyro.infer import MCMC, NUTS, SVI\n",
    "from numpyro.optim import Adam\n",
    "from numpyro.contrib.autoguide import AutoContinuousELBO, AutoDiagonalNormal\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(func, points, seed=0):\n",
    "    \"\"\"Generate a dataframe containing the covariate X, and observations Y\n",
    "    \"\"\"\n",
    "    numpy.random.seed(seed)\n",
    "\n",
    "    data = []\n",
    "    for segment in points:\n",
    "        x = numpy.linspace(*segment[\"xlim\"], num=segment[\"n_points\"])\n",
    "        distribution = func(x)\n",
    "        # Generate observations\n",
    "        y = distribution.rvs()\n",
    "        df = pd.DataFrame({\"x\": x, \"y\": y})\n",
    "        data.append(df)\n",
    "\n",
    "    return pd.concat(data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\"The non-linearity used in our neural network\n",
    "    \"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def feedforward(X, Y, width=5, hidden=1, sigma=1.0, noise=1.0):\n",
    "    \"\"\"An implementation of feedforward Bayesian neural network with a fixed width of hidden layers\n",
    "    and linear output node.\n",
    "    \"\"\"\n",
    "    if Y is not None:\n",
    "        assert Y.shape[1] == 1\n",
    "    DX, DY, DH = X.shape[1], 1, width\n",
    "\n",
    "    # Sample first layer\n",
    "    i = 0\n",
    "    w = numpyro.sample(f\"w{i}\", dist.Normal(np.zeros((DX, DH)), np.ones((DX, DH)) * sigma))\n",
    "    b = numpyro.sample(f\"b{i}\", dist.Normal(np.zeros((DX, DH)), np.ones((DX, DH)) * sigma))\n",
    "    z = activation(np.matmul(X, w) + b)  # N DH  <= first layer of activations\n",
    "\n",
    "    for i in range(1, hidden):\n",
    "        w = numpyro.sample(f\"w{i}\", dist.Normal(np.zeros((DH, DH)), np.ones((DH, DH)) * sigma))\n",
    "        b = numpyro.sample(f\"b{i}\", dist.Normal(np.zeros((1, DH)), np.ones((1, DH)) * sigma))\n",
    "        z = activation(np.matmul(z, w) + b)  # N DH  <= second layer of activations\n",
    "\n",
    "    # Sample final layer of weights and neural network output\n",
    "    i += 1\n",
    "    w = numpyro.sample(f\"w{i}\", dist.Normal(np.zeros((DH, DY)), np.ones((DH, DY)) * sigma))\n",
    "    b = numpyro.sample(f\"b{i}\", dist.Normal(0, sigma))\n",
    "    z = np.matmul(z, w) + b  # N DY  <= output of the neural network\n",
    "\n",
    "    # Likelihood\n",
    "    numpyro.sample(\"Y\", dist.Normal(z, noise), obs=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(\n",
    "    model, num_samples, num_warmup, num_chains, seed=0, chain_method=\"parallel\", summary=True, **kwargs\n",
    "):\n",
    "    \"\"\"Run the No-U-Turn sampler\n",
    "    \"\"\"\n",
    "    rng_key = random.PRNGKey(seed)\n",
    "    kernel = NUTS(model)\n",
    "    # Note: sampling more than one chain doesn't show a progress bar\n",
    "    mcmc = MCMC(kernel, num_warmup, num_samples, num_chains, chain_method=chain_method)\n",
    "    mcmc.run(rng_key, **kwargs)\n",
    "\n",
    "    if summary:\n",
    "        mcmc.print_summary()\n",
    "\n",
    "    # Return a fitted MCMC object\n",
    "    return mcmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADVIResults:\n",
    "    \"\"\"A convenience class to work with the results of Variational Inference\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, svi, guide, state, losses):\n",
    "        self.svi = svi\n",
    "        self.guide = guide\n",
    "        self.state = state\n",
    "        self.losses = losses\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"Obtain the parameters of the variational distribution\n",
    "        \"\"\"\n",
    "        return self.svi.get_params(self.state)\n",
    "\n",
    "    def sample_posterior(self, rng_key, n_samples):\n",
    "        \"\"\"Sample from the posterior, making all necessary transformations of\n",
    "        the reparametrized variational distribution.\n",
    "        \"\"\"\n",
    "        params = self.get_params()\n",
    "        posterior_samples = self.guide.sample_posterior(rng_key, params, sample_shape=(n_samples,))\n",
    "        return posterior_samples\n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.losses)\n",
    "        plt.yscale(\"log\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.title(f\"Negative ELBO by Iteration, Final Value {self.losses[-1]:.1f}\")\n",
    "\n",
    "\n",
    "def advi(model, num_iter, learning_rate=0.01, seed=0):\n",
    "    \"\"\"Automatic Differentiation Variational Inference using a Normal variational distribution\n",
    "    with a diagonal covariance matrix.\n",
    "    \"\"\"\n",
    "    rng_key = random.PRNGKey(seed)\n",
    "    adam = Adam(learning_rate)\n",
    "    # Automatically create a variational distribution (aka \"guide\" in Pyro's terminology)\n",
    "    guide = AutoDiagonalNormal(model)\n",
    "    svi = SVI(model, guide, adam, AutoContinuousELBO())\n",
    "    svi_state = svi.init(rng_key)\n",
    "\n",
    "    # Run optimization\n",
    "    last_state, losses = lax.scan(lambda state, i: svi.update(state), svi_state, np.zeros(num_iter))\n",
    "    results = ADVIResults(svi=svi, guide=guide, state=last_state, losses=losses)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, rng_key, samples, X):\n",
    "    \"\"\"Numpyro's helper function for prediction\n",
    "    \"\"\"\n",
    "    model = handlers.substitute(handlers.seed(model, rng_key), samples)\n",
    "    # note that Y will be sampled in the model because we pass Y=None here\n",
    "    model_trace = handlers.trace(model).get_trace(X=X, Y=None)\n",
    "    return model_trace[\"Y\"][\"value\"]\n",
    "\n",
    "\n",
    "def simulate_posterior_predictive(model, mcmc_or_vi, X_test, n_samples=None, seed=1, noiseless=False):\n",
    "    \"\"\"Predict Y_test at inputs X_test for a Numpyro model\n",
    "    \"\"\"\n",
    "    # Set random state\n",
    "    rng_key = random.PRNGKey(seed)\n",
    "\n",
    "    # Obtain samples of the posterior\n",
    "    if isinstance(mcmc_or_vi, MCMC):\n",
    "        posterior_samples = mcmc.get_samples()\n",
    "        n_samples = mcmc_or_vi.num_samples * mcmc_or_vi.num_chains\n",
    "    elif isinstance(mcmc_or_vi, ADVIResults):\n",
    "        assert n_samples is not None, \"The argument `n_samples` must be specified for Variational Inference\"\n",
    "        posterior_samples = mcmc_or_vi.sample_posterior(rng_key, n_samples)\n",
    "    else:\n",
    "        raise ValueError(\"The `mcmc_or_vi` argument must be of type MCMC or ADVIResults\")\n",
    "\n",
    "    # Generate samples from the posterior predictive\n",
    "    vmap_args = (posterior_samples, random.split(rng_key, n_samples))\n",
    "    predictions = vmap(lambda samples, rng_key: predict(model, rng_key, samples, X_test))(*vmap_args)\n",
    "    predictions = predictions[..., 0]\n",
    "\n",
    "    # Optionally, return mean predictions (the variance of which is epistemic uncertainty)\n",
    "    if noiseless:\n",
    "        raise NotImplemented(\"A model with zero noise should be passed instead\")\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_true_function(func, df, title=None):\n",
    "    x = numpy.linspace(df.x.min(), df.x.max(), num=1000)\n",
    "    distribution = func(x)\n",
    "    lower, upper = distribution.interval(0.95)\n",
    "\n",
    "    plt.fill_between(\n",
    "        x, lower, upper, color=\"tab:orange\", alpha=0.1, label=\"True 95% Interval\",\n",
    "    )\n",
    "    plt.scatter(df.x, df.y, s=10, color=\"lightgrey\", label=\"Observations\")\n",
    "    plt.plot(x, distribution.mean(), color=\"tab:red\", label=\"True Mean\")\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.legend(bbox_to_anchor=(1.04, 1), borderaxespad=0)\n",
    "\n",
    "\n",
    "def plot_posterior_predictive(x, y, title=None, func=None, df=None):\n",
    "    if func is not None and df is not None:\n",
    "        plot_true_function(func, df)\n",
    "\n",
    "    x = x.ravel()\n",
    "    lower, upper = numpy.percentile(y, [2.5, 97.5], axis=0)\n",
    "    plt.fill_between(x, lower, upper, color=\"tab:blue\", alpha=0.1, label=f\"Predicted 95% Interval\")\n",
    "    plt.plot(x, y.mean(axis=0), color=\"tab:blue\", label=f\"Predicted Mean\")\n",
    "    plt.title(title)\n",
    "    plt.legend(bbox_to_anchor=(1.04, 1), borderaxespad=0)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
